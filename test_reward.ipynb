{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import datetime as datetime\n",
    "import glob\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "import gym\n",
    "import gym_banana\n",
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency pair</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bid provider</th>\n",
       "      <th>bid price</th>\n",
       "      <th>bid volume</th>\n",
       "      <th>ask provider</th>\n",
       "      <th>ask price</th>\n",
       "      <th>ask volume</th>\n",
       "      <th>pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>02.01.2019 00:00:00</td>\n",
       "      <td>LP-3</td>\n",
       "      <td>0.72714</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72718</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>02.01.2019 00:00:01</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72712</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72717</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>02.01.2019 00:00:02</td>\n",
       "      <td>LP-1</td>\n",
       "      <td>0.72713</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72718</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>02.01.2019 00:00:03</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72713</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72717</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>02.01.2019 00:00:04</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72713</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>LP-2</td>\n",
       "      <td>0.72717</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  currency pair            timestamp bid provider  bid price  bid volume  \\\n",
       "0        AUDUSD  02.01.2019 00:00:00         LP-3    0.72714   1000000.0   \n",
       "1        AUDUSD  02.01.2019 00:00:01         LP-2    0.72712   2000000.0   \n",
       "2        AUDUSD  02.01.2019 00:00:02         LP-1    0.72713   1000000.0   \n",
       "3        AUDUSD  02.01.2019 00:00:03         LP-2    0.72713   1000000.0   \n",
       "4        AUDUSD  02.01.2019 00:00:04         LP-2    0.72713   2000000.0   \n",
       "\n",
       "  ask provider  ask price  ask volume  pad  \n",
       "0         LP-2    0.72718   2000000.0  0.0  \n",
       "1         LP-2    0.72717   2000000.0  0.0  \n",
       "2         LP-2    0.72718   2000000.0  0.0  \n",
       "3         LP-2    0.72717   2000000.0  0.0  \n",
       "4         LP-2    0.72717   1000000.0  0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pad = pd.read_csv('PadData_v2.csv')\n",
    "Pad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward for episode No.1 is 0.04295500000004049\n"
     ]
    }
   ],
   "source": [
    "T = 3617\n",
    "m = 10\n",
    "to_draw = np.sort(Pad['timestamp'].unique())\n",
    "ccy = np.sort(Pad['currency pair'].unique())\n",
    "min_history = 500 # min episode length\n",
    "\n",
    "    \n",
    "def generate_episode(n,cur):\n",
    "    _max = to_draw.shape[0]\n",
    "    _end = min(n+T, _max)\n",
    "    timeframe = to_draw[n:_end]\n",
    "    other_bid = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    other_ask = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    i = 0\n",
    "    for elem in ccy:\n",
    "        tmp = Pad[Pad['currency pair'] == elem]\n",
    "        if elem == cur:\n",
    "            target_bid = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            target_ask = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "        else:\n",
    "            other_bid[:,i] = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            other_ask[:,i] = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "            i += 1\n",
    "    return target_bid, target_ask, other_bid, other_ask\n",
    "\n",
    "def features(price_path,m):\n",
    "    features = np.zeros((price_path.shape[0]-m,m))\n",
    "    for i in range(m):\n",
    "        features[:,i] = (np.log(price_path) - np.log(np.roll(price_path, i+1)))[m:]\n",
    "    return features\n",
    "\n",
    "def get_features(target_bid, target_ask, other_bid, other_ask, m):\n",
    "    feature_span = features(target_bid,m)\n",
    "    feature_span = np.append(feature_span, features(target_ask,m), axis = 1)\n",
    "    for i in range(other_bid.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_bid[:,i],m), axis = 1)\n",
    "    for j in range(other_ask.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_ask[:,j],m), axis = 1)\n",
    "    return feature_span\n",
    "\n",
    "def draw_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "#     n = np.random.randint(to_draw.shape[0] - min_history)\n",
    "    n = 1\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    return target_bid, target_ask, feature_span\n",
    "\n",
    "def calculate_reward(previous_action, current_action):\n",
    "    pass\n",
    "\n",
    "def draw_episode_with_episode_num(m, cur, min_history, n = None):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "    if n is None:\n",
    "        n = np.random.randint(to_draw.shape[0] - min_history)\n",
    "    \n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    return target_bid, target_ask, feature_span\n",
    "    \n",
    "def test_policy(policy = None, episode=None, totalNumEpisodes=1):\n",
    "    if episode is None:\n",
    "        for i in range(totalNumEpisodes):\n",
    "            ask = np.zeros((1, 1))\n",
    "            bid = np.zeros((1,1 ))\n",
    "            previous_action = 0\n",
    "            reward_for_this_episode = 0\n",
    "            while ask.shape[0] <= 3600 and bid.shape[0]<=3600:\n",
    "                target_bid, target_ask, feature_span = draw_episode_with_episode_num(16, 'AUDUSD', 1000, 1)\n",
    "                bid, ask, features = target_bid[16:], target_ask[16:], feature_span\n",
    "            for t in range(3600):  # Don't infinite loop while learning\n",
    "                reward_for_this_episode += max(max(((ask[t+1] - ask[t]) + (bid[t+1] - bid[t]))/2, -((ask[t+1] - ask[t]) + (bid[t+1] - bid[t]))/2), 0)\n",
    "            print(\"The reward for episode No.{} is {}\".format(1, reward_for_this_episode))\n",
    "            \n",
    "        # a part of code to figure out the maximum value we can get\n",
    "                \n",
    "    else:\n",
    "        #run policy through episode\n",
    "        #compute\n",
    "        pass\n",
    "    \n",
    "def main():\n",
    "    test_policy()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-27 17:25:47,813 - root - INFO - BananaEnv - Version 0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Finished 0 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 1 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 2 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 3 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 4 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 5 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 6 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 7 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 8 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 9 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 10 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 11 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 12 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 13 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 14 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 15 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 16 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 17 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 18 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 19 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 20 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 21 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 22 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 23 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 24 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 25 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 26 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 27 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 28 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 29 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 30 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 31 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 32 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 33 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 34 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 35 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 36 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 37 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 38 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 39 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 40 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 41 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 42 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 43 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 44 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 45 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 46 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 47 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 48 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 49 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 50 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 51 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 52 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 53 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 54 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 55 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 56 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 57 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 58 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 59 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 60 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 61 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 62 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 63 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 64 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 65 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 66 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 67 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 68 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 69 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 70 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 71 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 72 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 73 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 74 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 75 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 76 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 77 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 78 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 79 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 80 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 81 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 82 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 83 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 84 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 85 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 86 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 87 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 88 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 89 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 90 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 91 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 92 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 93 episode and the policy loss is 0.0\n",
      "None\n",
      "Finished 94 episode and the policy loss is 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "T = 3617\n",
    "m = 10\n",
    "to_draw = np.sort(Pad['timestamp'].unique())\n",
    "ccy = np.sort(Pad['currency pair'].unique())\n",
    "min_history = 500 # min episode length\n",
    "\n",
    "    \n",
    "def generate_episode(n,cur):\n",
    "    _max = to_draw.shape[0]\n",
    "    _end = min(n+T, _max)\n",
    "    timeframe = to_draw[n:_end]\n",
    "    other_bid = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    other_ask = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    i = 0\n",
    "    for elem in ccy:\n",
    "        tmp = Pad[Pad['currency pair'] == elem]\n",
    "        if elem == cur:\n",
    "            target_bid = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            target_ask = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "        else:\n",
    "            other_bid[:,i] = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            other_ask[:,i] = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "            i += 1\n",
    "    return target_bid, target_ask, other_bid, other_ask\n",
    "\n",
    "def features(price_path,m):\n",
    "    features = np.zeros((price_path.shape[0]-m,m))\n",
    "    for i in range(m):\n",
    "        features[:,i] = (np.log(price_path) - np.log(np.roll(price_path, i+1)))[m:]\n",
    "    return features\n",
    "\n",
    "def get_features(target_bid, target_ask, other_bid, other_ask, m):\n",
    "    feature_span = features(target_bid,m)\n",
    "    feature_span = np.append(feature_span, features(target_ask,m), axis = 1)\n",
    "    for i in range(other_bid.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_bid[:,i],m), axis = 1)\n",
    "    for j in range(other_ask.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_ask[:,j],m), axis = 1)\n",
    "    return feature_span\n",
    "\n",
    "def draw_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "#     n = np.random.randint(to_draw.shape[0] - min_history)\n",
    "    n = 1\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    return target_bid, target_ask, feature_span\n",
    "import gym\n",
    "import gym_banana\n",
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                     help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                     help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true',\n",
    "#                     help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='interval between training status logs (default: 10)')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('Banana-v0')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        #self.affine1 = nn.Linear(256, 3)\n",
    "        self.A = Variable(torch.randn(256, 3), requires_grad=True)\n",
    "        self.b = Variable(torch.randn(1), requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform(self.affine1.weight)\n",
    "        #print('The weight of the affine layer is', self.affine1.weight)\n",
    "#         self.dropout = nn.Dropout(p=0)\n",
    " #       self.mu = Variable(torch.randn(3, 3), requires_grad=True)\n",
    "        #self.mu = nn.Linear(1,3, bias = True)\n",
    "                #torch.nn.init.xavier_uniform(self.affine1.weight)\n",
    "        #print('The weight of the affine layer is', self.affine1.weight)\n",
    "        # self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.matmul(x, self.A) \n",
    "#         x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        #mu_sigma = torch.matmul(y, self.mu)\n",
    "#         print(mu_sigma.size())\n",
    "        # action = self.tanh(x + mu_sigma)\n",
    "        # action = self.softmax(action)\n",
    "        action = self.softmax(x)\n",
    "#         action_scores = self.affine2(x)\n",
    "#         return F.softmax(action_scores, dim=1)\n",
    "        return action\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "\n",
    "optimizer = optim.SGD([policy.A, policy.b], lr=1e-1)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state, previous_action):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    previous_action = torch.from_numpy(previous_action).float()\n",
    "    probs = policy(state, previous_action)\n",
    "    #return probs\n",
    "    #m = Categorical(probs)\n",
    "    #action = m.sample()\n",
    "    #policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return probs.detach().numpy()\n",
    "    #return action.item() - 1\n",
    "\n",
    "\n",
    "def finish_episode(num):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        #print(\"reward\",r * 1000000)\n",
    "#         R = r\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    optimizer.zero_grad()\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for R in returns:\n",
    "#         print(Variable(-torch.ones(log_prob.shape), requires_grad = True) * R)\n",
    "        #print(\"return\", R)\n",
    "        #print(\"policy_append\", Variable(-torch.ones(log_prob.shape) * 1000000, requires_grad = True) * R)\n",
    "        #policy_loss.append( torch.tensor(-1000000* R))\n",
    "        policy_loss.append(Variable(-torch.ones((1,)) * 1000000, requires_grad = True) * R)\n",
    " \n",
    "    policy_loss = torch.cat(policy_loss).sum() \n",
    "    #policy_loss = policy.A.sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(policy.A.grad)\n",
    "    #print(policy.b.grad)\n",
    "    #print(list(policy.parameters()))\n",
    "    print (\"Finished {} episode and the policy loss is {}\".format(num, policy_loss))\n",
    "    policy.rewards = []\n",
    "    #optimizer.zero_grad()\n",
    "    #del policy.saved|_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i_episode in range(100):\n",
    "        ask = np.zeros((1, 1))\n",
    "        bid = np.zeros((1,1 ))\n",
    "        previous_action = np.array([0, 0, 1])\n",
    "        while ask.shape[0] <= 3600 and bid.shape[0]<=3600:\n",
    "            target_bid, target_ask, feature_span = draw_episode(16, 'AUDUSD', 1000)\n",
    "            bid, ask, features = target_bid[16:], target_ask[16:], feature_span\n",
    "        for t in range(3600):  # Don't infinite loop while learning\n",
    "            state = feature_span[t]\n",
    "            action = select_action(state, previous_action)\n",
    "            reward = ((action) * ((ask[t+1] - ask[t]) + (bid[t+1] - bid[t]))/2).sum()\n",
    "            #reward -= 0.01 * abs(previous_action - action)  \n",
    "            # the current price is \n",
    "#             state, reward, done, _ = env.step(action) #change banana\n",
    "  #          if args.render:\n",
    "  #              env.render()\n",
    "            #print('reward is', reward)\n",
    "            policy.rewards.append(reward)\n",
    "#             ep_reward += reward\n",
    "            #policy.actions.append(action)\n",
    "            previous_action = action\n",
    "\n",
    "\n",
    "#         running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode(i_episode)\n",
    "#         if i_episode % 5 == 0:\n",
    "#             print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "#                   i_episode, ep_reward, running_reward))\n",
    "#         if running_reward > env.spec.reward_threshold:\n",
    "#             print(\"Solved! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "#             break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for param in policy.parameters():\n",
    "    print((param.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0322, -0.0276, -0.0121,  0.0293, -0.0588,  0.0375, -0.0129,  0.0318,\n",
      "          0.0087, -0.0077,  0.0173,  0.0031,  0.0228, -0.0244, -0.0046, -0.0056,\n",
      "          0.0091, -0.0002,  0.0546,  0.0194, -0.0233, -0.0377, -0.0105, -0.0270,\n",
      "         -0.0200,  0.0030,  0.0373,  0.0340, -0.0611,  0.0387,  0.0175,  0.0593,\n",
      "          0.0413, -0.0569, -0.0594, -0.0301,  0.0549, -0.0104,  0.0267, -0.0290,\n",
      "          0.0613, -0.0264,  0.0469,  0.0007, -0.0329,  0.0321, -0.0332,  0.0184,\n",
      "         -0.0180, -0.0069, -0.0601, -0.0298,  0.0339, -0.0152,  0.0623,  0.0501,\n",
      "         -0.0029, -0.0417,  0.0381,  0.0194, -0.0404,  0.0406,  0.0379,  0.0554,\n",
      "         -0.0350, -0.0103, -0.0012,  0.0091, -0.0474, -0.0444,  0.0340, -0.0147,\n",
      "          0.0305,  0.0036,  0.0205,  0.0137,  0.0227,  0.0310, -0.0579,  0.0315,\n",
      "         -0.0439, -0.0472,  0.0038, -0.0107,  0.0367, -0.0362, -0.0556,  0.0455,\n",
      "         -0.0093,  0.0352,  0.0201, -0.0469,  0.0126,  0.0150, -0.0418, -0.0297,\n",
      "          0.0213,  0.0112, -0.0266, -0.0189,  0.0572, -0.0116,  0.0352,  0.0271,\n",
      "         -0.0404, -0.0532,  0.0600,  0.0033,  0.0428,  0.0130,  0.0201,  0.0467,\n",
      "          0.0593, -0.0415,  0.0078,  0.0466,  0.0453,  0.0388, -0.0452, -0.0450,\n",
      "         -0.0378,  0.0079,  0.0623, -0.0395,  0.0333, -0.0346, -0.0588, -0.0133,\n",
      "          0.0360,  0.0580, -0.0388,  0.0136,  0.0539,  0.0414,  0.0390,  0.0444,\n",
      "          0.0395,  0.0161, -0.0427, -0.0525, -0.0286, -0.0073, -0.0383,  0.0229,\n",
      "          0.0193, -0.0142,  0.0240,  0.0202,  0.0382,  0.0421, -0.0212,  0.0611,\n",
      "         -0.0072, -0.0021, -0.0590, -0.0402, -0.0365, -0.0267,  0.0444, -0.0204,\n",
      "         -0.0467,  0.0240,  0.0200,  0.0405, -0.0323,  0.0136, -0.0228, -0.0140,\n",
      "         -0.0498, -0.0285, -0.0191,  0.0267,  0.0114,  0.0154,  0.0624,  0.0609,\n",
      "          0.0426,  0.0020, -0.0432,  0.0488, -0.0156, -0.0051, -0.0538, -0.0123,\n",
      "         -0.0403,  0.0574, -0.0540, -0.0487, -0.0021, -0.0338,  0.0224, -0.0241,\n",
      "         -0.0294,  0.0035,  0.0452, -0.0440,  0.0294,  0.0402,  0.0611, -0.0437,\n",
      "          0.0151, -0.0462,  0.0534, -0.0242,  0.0376,  0.0019, -0.0049, -0.0020,\n",
      "          0.0106,  0.0295,  0.0100,  0.0191, -0.0562,  0.0455,  0.0545,  0.0517,\n",
      "          0.0462, -0.0451, -0.0232,  0.0551, -0.0476,  0.0567, -0.0492, -0.0440,\n",
      "          0.0306, -0.0449, -0.0143,  0.0455,  0.0495,  0.0591, -0.0127, -0.0486,\n",
      "          0.0615, -0.0133, -0.0257,  0.0152, -0.0437,  0.0411,  0.0392, -0.0496,\n",
      "         -0.0513, -0.0055,  0.0263, -0.0018, -0.0317,  0.0014, -0.0587, -0.0442,\n",
      "         -0.0416,  0.0515,  0.0551, -0.0212,  0.0028,  0.0279,  0.0075, -0.0313],\n",
      "        [ 0.0360,  0.0384, -0.0036, -0.0077,  0.0568,  0.0546, -0.0354,  0.0611,\n",
      "          0.0155, -0.0415,  0.0342, -0.0467,  0.0577, -0.0402,  0.0177,  0.0190,\n",
      "          0.0149,  0.0518, -0.0260, -0.0264, -0.0542, -0.0026, -0.0296,  0.0025,\n",
      "         -0.0128,  0.0207,  0.0541,  0.0184, -0.0201, -0.0307, -0.0545,  0.0526,\n",
      "         -0.0118,  0.0126,  0.0023, -0.0398,  0.0352,  0.0350,  0.0031, -0.0355,\n",
      "         -0.0266,  0.0009, -0.0357, -0.0349, -0.0093, -0.0180,  0.0153, -0.0164,\n",
      "         -0.0083, -0.0239, -0.0571,  0.0545,  0.0119,  0.0559, -0.0101, -0.0157,\n",
      "         -0.0588, -0.0237,  0.0410, -0.0612, -0.0219,  0.0381,  0.0360,  0.0231,\n",
      "         -0.0124, -0.0047, -0.0119, -0.0230,  0.0261, -0.0213, -0.0351,  0.0142,\n",
      "         -0.0452, -0.0133,  0.0319,  0.0234,  0.0532, -0.0426,  0.0005, -0.0185,\n",
      "         -0.0350, -0.0352,  0.0157,  0.0065, -0.0319, -0.0441, -0.0518,  0.0613,\n",
      "         -0.0107, -0.0145, -0.0230, -0.0313, -0.0572, -0.0367,  0.0382,  0.0137,\n",
      "         -0.0220, -0.0236,  0.0399,  0.0451,  0.0602,  0.0182,  0.0302, -0.0050,\n",
      "         -0.0365, -0.0612,  0.0381, -0.0091,  0.0257,  0.0031, -0.0583, -0.0307,\n",
      "          0.0176, -0.0127, -0.0604,  0.0219,  0.0024, -0.0201,  0.0252,  0.0132,\n",
      "         -0.0231,  0.0561,  0.0314,  0.0078, -0.0120, -0.0529,  0.0483, -0.0161,\n",
      "         -0.0404,  0.0401,  0.0222,  0.0155, -0.0124, -0.0195,  0.0106,  0.0484,\n",
      "          0.0090, -0.0296,  0.0621,  0.0013,  0.0494,  0.0152,  0.0560,  0.0361,\n",
      "         -0.0319,  0.0439,  0.0004, -0.0502, -0.0131, -0.0564, -0.0324, -0.0443,\n",
      "          0.0427, -0.0131, -0.0108,  0.0300,  0.0254, -0.0412,  0.0230,  0.0266,\n",
      "         -0.0497,  0.0543, -0.0380, -0.0244, -0.0515,  0.0571,  0.0119,  0.0526,\n",
      "          0.0041,  0.0340, -0.0200, -0.0499,  0.0100, -0.0056, -0.0063, -0.0307,\n",
      "          0.0013,  0.0179,  0.0225, -0.0503,  0.0302,  0.0064,  0.0431,  0.0442,\n",
      "         -0.0492,  0.0600, -0.0615,  0.0359,  0.0044,  0.0296, -0.0338,  0.0376,\n",
      "         -0.0309, -0.0552,  0.0209,  0.0342,  0.0620, -0.0065,  0.0601,  0.0402,\n",
      "         -0.0310, -0.0482,  0.0351, -0.0342,  0.0287, -0.0535,  0.0456,  0.0187,\n",
      "         -0.0072,  0.0287, -0.0445, -0.0137,  0.0006, -0.0361, -0.0160, -0.0270,\n",
      "          0.0596,  0.0531,  0.0539,  0.0511, -0.0599,  0.0209,  0.0230, -0.0458,\n",
      "          0.0253, -0.0339,  0.0099,  0.0207,  0.0539, -0.0328, -0.0492,  0.0578,\n",
      "          0.0262,  0.0304,  0.0225, -0.0221,  0.0376, -0.0546,  0.0087,  0.0283,\n",
      "          0.0427, -0.0312, -0.0422,  0.0234, -0.0496, -0.0519, -0.0562, -0.0303,\n",
      "         -0.0119, -0.0370, -0.0264, -0.0553,  0.0384, -0.0143, -0.0394,  0.0163],\n",
      "        [-0.0467,  0.0437,  0.0359, -0.0614, -0.0491, -0.0412, -0.0338,  0.0078,\n",
      "         -0.0618, -0.0463,  0.0285, -0.0205, -0.0249, -0.0517,  0.0253,  0.0044,\n",
      "         -0.0242, -0.0383,  0.0133,  0.0170,  0.0277,  0.0186, -0.0095, -0.0484,\n",
      "          0.0398, -0.0005, -0.0415,  0.0539,  0.0348, -0.0491, -0.0312, -0.0213,\n",
      "         -0.0361,  0.0292, -0.0446,  0.0581, -0.0258,  0.0369,  0.0021, -0.0275,\n",
      "          0.0417, -0.0477, -0.0331,  0.0075,  0.0496, -0.0268, -0.0381, -0.0399,\n",
      "         -0.0276, -0.0216, -0.0146, -0.0356,  0.0195,  0.0005, -0.0408, -0.0357,\n",
      "          0.0132, -0.0009,  0.0442, -0.0095, -0.0506, -0.0462, -0.0183, -0.0138,\n",
      "          0.0071, -0.0140,  0.0231, -0.0208,  0.0459,  0.0144, -0.0192,  0.0557,\n",
      "          0.0160, -0.0425, -0.0348,  0.0392,  0.0285,  0.0488, -0.0048,  0.0437,\n",
      "          0.0311,  0.0193, -0.0145,  0.0602,  0.0126, -0.0161, -0.0009,  0.0614,\n",
      "          0.0420, -0.0046,  0.0613,  0.0274, -0.0333, -0.0569,  0.0363,  0.0586,\n",
      "         -0.0364,  0.0257,  0.0027, -0.0134, -0.0170,  0.0174,  0.0138,  0.0422,\n",
      "          0.0229,  0.0284,  0.0551,  0.0195,  0.0093, -0.0228,  0.0458,  0.0092,\n",
      "          0.0403,  0.0604, -0.0140,  0.0440, -0.0127,  0.0564,  0.0492, -0.0224,\n",
      "          0.0022, -0.0273, -0.0253, -0.0127, -0.0004, -0.0218, -0.0590, -0.0378,\n",
      "         -0.0053, -0.0200,  0.0464,  0.0508,  0.0398,  0.0277, -0.0495,  0.0590,\n",
      "         -0.0551, -0.0205, -0.0440,  0.0225, -0.0520, -0.0308, -0.0397, -0.0384,\n",
      "         -0.0292,  0.0614, -0.0051,  0.0191, -0.0620,  0.0556, -0.0136, -0.0574,\n",
      "          0.0431, -0.0169,  0.0568, -0.0425, -0.0580, -0.0470,  0.0271, -0.0070,\n",
      "         -0.0579,  0.0116, -0.0040,  0.0226, -0.0318, -0.0446,  0.0426, -0.0301,\n",
      "          0.0440,  0.0507, -0.0437, -0.0288,  0.0075, -0.0589,  0.0192, -0.0260,\n",
      "         -0.0277, -0.0483,  0.0292, -0.0398, -0.0500, -0.0164, -0.0348, -0.0608,\n",
      "          0.0252, -0.0238,  0.0438,  0.0440, -0.0399, -0.0364, -0.0222,  0.0366,\n",
      "         -0.0022, -0.0223,  0.0387,  0.0593,  0.0462, -0.0478, -0.0130, -0.0120,\n",
      "          0.0533, -0.0323, -0.0358,  0.0010, -0.0145,  0.0237,  0.0076, -0.0373,\n",
      "          0.0454,  0.0389,  0.0137, -0.0566,  0.0294,  0.0415,  0.0174, -0.0242,\n",
      "          0.0042, -0.0240,  0.0588,  0.0604, -0.0073, -0.0420,  0.0368,  0.0319,\n",
      "          0.0331, -0.0470,  0.0212, -0.0267, -0.0549, -0.0436,  0.0265,  0.0234,\n",
      "         -0.0549,  0.0354,  0.0479,  0.0474,  0.0353, -0.0098,  0.0419,  0.0489,\n",
      "          0.0326, -0.0450, -0.0218,  0.0495,  0.0476,  0.0399, -0.0261, -0.0124,\n",
      "         -0.0466, -0.0356,  0.0480, -0.0513, -0.0154,  0.0133, -0.0375,  0.0146]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0100,  0.0250, -0.0480], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1380],\n",
      "        [-0.9117],\n",
      "        [ 0.6204]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.9714, -0.3569,  0.1479], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(policy.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
