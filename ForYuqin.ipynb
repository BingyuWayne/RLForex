{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3617\n",
    "m = 10\n",
    "to_draw = np.sort(Pad['timestamp'].unique())\n",
    "ccy = np.sort(Pad['currency pair'].unique())\n",
    "min_history = 500 # min episode length\n",
    "\n",
    "    \n",
    "def generate_episode(n,cur):\n",
    "    _max = to_draw.shape[0]\n",
    "    _end = min(n+T, _max)\n",
    "    timeframe = to_draw[n:_end]\n",
    "    other_bid = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    other_ask = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    i = 0\n",
    "    for elem in ccy:\n",
    "        tmp = Pad[Pad['currency pair'] == elem]\n",
    "        if elem == cur:\n",
    "            target_bid = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            target_ask = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "        else:\n",
    "            other_bid[:,i] = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            other_ask[:,i] = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "            i += 1\n",
    "    return target_bid, target_ask, other_bid, other_ask\n",
    "\n",
    "def features(price_path,m):\n",
    "    features = np.zeros((price_path.shape[0]-m,m))\n",
    "    for i in range(m):\n",
    "        features[:,i] = (np.log(price_path) - np.log(np.roll(price_path, i+1)))[m:]\n",
    "    return features\n",
    "\n",
    "def get_features(target_bid, target_ask, other_bid, other_ask, m):\n",
    "    feature_span = features(target_bid,m)\n",
    "    feature_span = np.append(feature_span, features(target_ask,m), axis = 1)\n",
    "    for i in range(other_bid.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_bid[:,i],m), axis = 1)\n",
    "    for j in range(other_ask.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_ask[:,j],m), axis = 1)\n",
    "    return feature_span\n",
    "\n",
    "def draw_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "#     n = np.random.randint(to_draw.shape[0] - min_history)\n",
    "    n = 1\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    return target_bid, target_ask, feature_span\n",
    "import gym\n",
    "import gym_banana\n",
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                     help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                     help='random seed (default: 543)')\n",
    "# parser.add_argument('--render', action='store_true',\n",
    "#                     help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='interval between training status logs (default: 10)')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('Banana-v0')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(256, 3)\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.mu = nn.Linear(1,3, bias = True)\n",
    "        # self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        mu_sigma = self.mu(y)\n",
    "#         print(mu_sigma.size())\n",
    "        # action = self.tanh(x + mu_sigma)\n",
    "        # action = self.softmax(action)\n",
    "        action = self.softmax(x + mu_sigma)\n",
    "#         action_scores = self.affine2(x)\n",
    "#         return F.softmax(action_scores, dim=1)\n",
    "        return action\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state, previous_action):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    previous_action = torch.from_numpy(np.array([previous_action])).float().unsqueeze(0)\n",
    "    probs = policy(state, previous_action)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item() - 1\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "#         print(Variable(-torch.ones(log_prob.shape), requires_grad = True) * R)\n",
    "        policy_loss.append(Variable(-torch.ones(log_prob.shape), requires_grad = True) * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print (\"Finished one episode and the policy loss is {}\".format(policy_loss))\n",
    "    #del policy.rewards[:]\n",
    "    #del policy.saved|_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i_episode in count(100):\n",
    "        ask = np.zeros((1, 1))\n",
    "        bid = np.zeros((1,1 ))\n",
    "        previous_action = 0\n",
    "        while ask.shape[0] <= 3600 and bid.shape[0]<=3600:\n",
    "            target_bid, target_ask, feature_span = draw_episode(16, 'AUDUSD', 1000)\n",
    "            bid, ask, features = target_bid[16:], target_ask[16:], feature_span\n",
    "        for t in range(3600):  # Don't infinite loop while learning\n",
    "            state = feature_span[t]\n",
    "            action = select_action(state, previous_action)\n",
    "            reward = (action) * ((ask[t+1] - ask[t]) + (bid[t+1] - bid[t]))/2\n",
    "            #reward -= 0.01 * abs(previous_action - action)  \n",
    "            # the current price is \n",
    "#             state, reward, done, _ = env.step(action) #change banana\n",
    "  #          if args.render:\n",
    "  #              env.render()\n",
    "            #print('reward is', reward)\n",
    "            policy.rewards.append(reward)\n",
    "#             ep_reward += reward\n",
    "            #policy.actions.append(action)\n",
    "            previous_action = action\n",
    "\n",
    "\n",
    "#         running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "#         if i_episode % 5 == 0:\n",
    "#             print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "#                   i_episode, ep_reward, running_reward))\n",
    "#         if running_reward > env.spec.reward_threshold:\n",
    "#             print(\"Solved! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "#             break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
