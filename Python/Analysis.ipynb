{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from run_deep_gpu import train_eval\n",
    "from test_deep_gpu import test\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "reward_file = './deep/reward'+ str(time.time()) + '.txt'\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--mode', type=str, default='train_eval')\n",
    "# parser.add_argument('--save', type=str, default='RLFX')\n",
    "\n",
    "# parser.add_argument('--reward_file', type=str, default=reward_file)\n",
    "\n",
    "\n",
    "# parser.add_argument('--currency', type=str, default='AUDUSD')\n",
    "# parser.add_argument('--min_history', type=int, default=1000)\n",
    "# parser.add_argument('--timespan', type=int, default=3600)\n",
    "# parser.add_argument('--lag', type=int, default=32)\n",
    "# parser.add_argument('--num_of_eval', type=int, default=25)\n",
    "\n",
    "# parser.add_argument('--init_lr', type=float, default=1e-1)\n",
    "# parser.add_argument('--num_of_epoch', type=int, default=150)\n",
    "# parser.add_argument('--num_of_episode', type=int, default=50)\n",
    "\n",
    "# parser.add_argument('--week_num', type=int, default=1)\n",
    "\n",
    "# parser.add_argment('--model_path', type=str, default='best_model_AUDUSD_week1_1559273823.5407195_dropout.pth')\n",
    "# parser.add_argument('--offset', type=int, default=300)\n",
    "# parser.add_argument('--num_of_test', type=int, default=50)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.mode = 'train_eval'\n",
    "        self.save = 'RLFX'\n",
    "        self.reward_file = reward_file\n",
    "        self.currency = 'AUDUSD'\n",
    "        self.min_history = 1000\n",
    "        self.timespan = 3600\n",
    "        self.lag = 32\n",
    "        self.num_of_eval = 25\n",
    "        self.init_lr = 1e-1\n",
    "        self.num_of_epoch = 150\n",
    "        self.num_of_episode = 25\n",
    "        self.week_num= 1\n",
    "        self.model_path = 'best_model_AUDUSD_week1_1559273823.5407195_dropout.pth'\n",
    "        self.offset = 300\n",
    "        self.num_of_test = 50\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_RF_RATE = 2.68 * 10 / 365.0 / 24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from run_deep_gpu import Policy\n",
    "#*********************************#\n",
    "from utils_deep import draw_eval_episode\n",
    "#*********************************#\n",
    "\n",
    "\n",
    "def test(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    policy = Policy()\n",
    "    if torch.cuda.is_available():\n",
    "        policy.load_state_dict(torch.load(config.model_path))\n",
    "    else:\n",
    "        policy.load_state_dict(torch.load(config.model_path, map_location='cpu'))\n",
    "    policy.to(device)\n",
    "\n",
    "    d = {'bid_price': [0], 'ask_price': [0], 'action':[0]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    rewards_over_time = []\n",
    "    g = np.zeros((config.num_of_test, 256))\n",
    "    with torch.no_grad():\n",
    "        accumulative_reward_test = 0\n",
    "        print_reward = []\n",
    "        for j in range(config.num_of_test):\n",
    "            current_reward = 0\n",
    "            ask = np.zeros((1, 1))\n",
    "            bid = np.zeros((1, 1))\n",
    "            previous_action = torch.tensor([0.0]).to(device)\n",
    "            while ask.shape[0] <= config.timespan and bid.shape[0]<=3600:\n",
    "                target_bid, target_ask, feature_span = draw_eval_episode(config.week_num, config.lag, config.currency,\n",
    "                                                                         config.min_history, j, config.offset)\n",
    "                # target_bid, target_ask, feature_span = draw_eval_episode(config.lag, config.currency,\n",
    "                #                                                          config.min_history, j, config.offset)\n",
    "                bid, ask, feature_span = target_bid*1e3, target_ask*1e3, feature_span\n",
    "            gradient = [0] * 256\n",
    "            for t in range(config.timespan):  # Don't infinite loop while learning\n",
    "                state = feature_span[t]\n",
    "                save_action = policy(state.float(),0.1*previous_action).to(device)\n",
    "                \n",
    "                for i in range(len(state)):\n",
    "                    state[i] += 0.00001\n",
    "                    save_action_grad = policy(state.float(),0.1*previous_action).to(device)\n",
    "                    gradient[i] += (save_action_grad - save_action)/0.00001\n",
    "                    state[i] -= 0.00001\n",
    "                if t == config.timespan-1:\n",
    "                    save_action = 0\n",
    "                action = save_action - previous_action\n",
    "\n",
    "                price = 0\n",
    "                if action > 0:\n",
    "                    price = ask[t]\n",
    "                elif action < 0:\n",
    "                    price = bid[t]\n",
    "                reward = torch.sum(torch.tensor(-1.).float() * action * price).to(device)\n",
    "                accumulative_reward_test += reward\n",
    "                current_reward  += reward\n",
    "\n",
    "                d = {'bid_price': [bid[t]], 'ask_price': [ask[t]], 'action':[action.item()]}\n",
    "                temp_df = pd.DataFrame(data=d)\n",
    "                df = df.append(temp_df)\n",
    "                previous_action = save_action\n",
    "            print_reward.append(current_reward)\n",
    "           \n",
    "            print(\"episode_reward\",current_reward)\n",
    "            gradient = [i / 3600.0 for i in gradient]\n",
    "            gradient = np.array(gradient)\n",
    "            g[j] = (gradient)\n",
    "        #get the standard deviation of print reward\n",
    "        g_avg = np.mean(g, axis=0)\n",
    "        reward_np = np.array(print_reward)\n",
    "        std = np.std(reward_np)\n",
    "        sharpe_ratio = (np.mean(reward_np) - HOURLY_RF_RATE) / std\n",
    "        print (\"Testing on {} datapoint and return is {}\".format(config.num_of_test, accumulative_reward_test))\n",
    "        rewards_over_time.append(accumulative_reward_test)\n",
    "\n",
    "    # Save the csv file\n",
    "    currency_pair = config.model_path[-27:-21]\n",
    "    \n",
    "    saved_path = 'deep/result/' + currency_pair + '_week_' + str(config.week_num) +'.csv'\n",
    "    # saved_path = currency_pair + '_train.csv'\n",
    "    print('Saving the csv file ...')\n",
    "    df.to_csv(saved_path, index = False)\n",
    "    np.save('all_gradient.npy', g)\n",
    "    print (sharpe_ratio)\n",
    "\n",
    "test(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
